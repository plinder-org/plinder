{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Access _PLINDER_ data for training ML models\n",
    "\n",
    "The goal of this tutorial is to provide a simple and simple hands-on demo for a new user to access _PLINDER_ dataset in prepparation for training machine learning models.\n",
    "\n",
    "Here, we are going to demonstrate how to get the key input data:\n",
    "- protein receptor fasta sequence\n",
    "- small molecules ligand SMILES string\n",
    "- access to linked _apo_ and _pred_ structure\n",
    "\n",
    "\n",
    "In the process, we will show:\n",
    "- How to query _PLINDER_ index and splits to select relevant data using `plinder.core` API\n",
    "- Extract task-specific data one might want to use for training a task-specific ML model, eg. one protein, one ligand\n",
    "- How to use `plinder.core` API and `PlinderDataset` class to supply dataset inputs for `train` or `val` splits\n",
    "- Load linked `apo` and `pred` structures\n",
    "- Example how to create a simple diversity sampler based on cluster labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Loading _PLINDER_\n",
    "\n",
    "We recommend users interact with the dataset using _PLINDER_ Python API.\n",
    "\n",
    "To install the API run: ``pip install plinder.[loader]``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from __future__ import annotations\n",
    "import pandas as pd\n",
    "from plinder.core.scores import query_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load _PLINDER_ index with selected columns from annotations table\n",
    "For a full list with descriptions, please refer to [docs](https://plinder-org.github.io/plinder/dataset.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get plinder index with selected annotation columns specified \n",
    "plindex = query_index(\n",
    "    columns=[\"system_id\", \"ligand_id\", \"ligand_rdkit_canonical_smiles\", \"ligand_is_ion\", \"ligand_is_artifact\", \"system_num_ligand_chains\", \"system_num_neighboring_protein_chains\"],\n",
    "    filters=[\n",
    "        (\"system_type\", \"==\", \"holo\"),\n",
    "        (\"system_num_neighboring_protein_chains\", \"<=\", 5)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plindex.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plindex.groupby(\"system_num_neighboring_protein_chains\").system_id.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting specific data using _PLINDER_ annotations\n",
    "As we can see just from the data tables above - a significant fraction of _PLINDER_ systems contain complex multi protein chain systems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task specific selection\n",
    "If we would like to focus on single protein and single ligand systems for training, we can use the annotated columns to filter out systems that:\n",
    "- contain only one protein chain\n",
    "- only one ligand\n",
    "\n",
    "Remember: In _PLINDER_ artifacts and (single atom) ions are also included in the index if they are part of the pocket.\n",
    "- We can use columns `ligand_is_ion` and `ligand_is_artifact` to only select \"proper\" ligands.\n",
    "\n",
    "Let's find out how many annotated ligands are \"proper\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define \"proper\" ligands that are not ions or artifacts\n",
    "plindex[\"ligand_is_proper\"] = (\n",
    "    ~plindex[\"ligand_is_ion\"] & ~plindex[\"ligand_is_artifact\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plindex.groupby(\"ligand_is_proper\").system_id.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### User choice\n",
    "\n",
    "The annotations table gives flexibility to choose the systems for training:\n",
    "- One could strictly choose to use only the data that contains single protein single ligand systems\n",
    "- Alternatively one could expand the number of systems to include systems containing single proper ligands, and optionally ignore the artifacts and ions in the pocket\n",
    "\n",
    "Let's compare the numbers of such systems!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create mask for single receptor single ligand systems\n",
    "systems_1P1L = (plindex[\"system_num_neighboring_protein_chains\"] == 1) & (plindex[\"system_num_ligand_chains\"] == 1)\n",
    "\n",
    "# make count of these \"proper\" ligands per system\n",
    "plindex[\"system_proper_num_ligand_chains\"] = plindex.groupby(\"system_id\")[\"ligand_is_proper\"].transform(\"sum\")\n",
    "\n",
    "# create mask only for single receptor single \"proper\" ligand systems\n",
    "systems_proper_1P1L = (plindex[\"system_num_neighboring_protein_chains\"] == 1) & (plindex[\"system_proper_num_ligand_chains\"] == 1) & plindex[\"ligand_is_proper\"]\n",
    "\n",
    "print(f\"Number of single receptor single ligand systems: {sum(systems_1P1L)}\")\n",
    "print(f\"Number of single receptor single \\\"proper\\\" ligand systems: {sum(systems_proper_1P1L)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see - the second choice can provide up to 20% more data for training, however, the caveat is that some of the interactions made by artifacts or ions may influence the binding pose of the \"proper\" ligand. The user could come up with further strategies to filtering using annotations table or external tools, but this is beyond the scope of this tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading splits\n",
    "\n",
    "Now, after curating the systems of interest, let's have a look at the splits using _PLINDER_ API.\n",
    "\n",
    "- How to use `plinder.core` API and `PlinderDataset` class to supply dataset inputs for `train` or `val` splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plinder.core import get_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting the splits\n",
    "The `get_split` function provided the current _PLINDER_ split, the detailed description of this DataFrame is provide in the [dataset documentation](https://plinder-org.github.io/plinder/dataset.html#splits-splits), but for our practical purposes we are mostly interested in `system_id` and `split` that assigns each of our systems to a specific split category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the current plinder split\n",
    "split_df = get_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some specific method developers working on _flexible_ docking may also find handy the annotation column `system_has_apo_or_pred` indicating if the system has available `apo` or `pred` linked structures (see later)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_df.groupby([\"split\", \"system_has_apo_or_pred\"]).system_id.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For simplicity let's merge plindex and split DataFrames into one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge to a single DataFrame\n",
    "plindex_split = plindex.merge(split_df, on=\"system_id\", how=\"left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting links to `apo` or `pred` structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plinder.core.scores import query_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links_df = query_links(\n",
    "    columns=[\"reference_system_id\", \"id\", \"sort_score\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the table is sorted by `sort_score` that is resolution for `apo`s and `plddt` for `pred`s. The `apo` or `pred` is specified in the additionally added `kind` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If a user wants to consider only one linked structure per system - we can easily drop duplicates, first sorting by `sort_score`. Using this priority score, `pred` structures will not be used unless there is no `apo` available. Alternative can be achieved by sorting with `ascending=False`, or filtering by `kind==\"pred\"` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_links_df = links_df.sort_values(\"sort_score\", ascending=True).drop_duplicates(\"reference_system_id\")\n",
    "single_links_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have links to `apo` / `pred` structures, we can see how many of those are available for our single protein single ligand systems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plindex_split[systems_1P1L].groupby([\"split\", \"system_has_apo_or_pred\"]).system_id.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plindex_split_1P1L_links = plindex_split[systems_1P1L].merge(single_links_df, left_on=\"system_id\", right_on=\"reference_system_id\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's check how many systems have linked structures\n",
    "plindex_split_1P1L_links['system_has_linked_apo_or_pred'] = ~plindex_split_1P1L_links.filename.isna()\n",
    "plindex_split_1P1L_links.groupby([\"split\", \"system_has_linked_apo_or_pred\"]).system_id.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: fix inconsistency\n",
    "plindex_split_1P1L_links.groupby([\"system_has_apo_or_pred\", \"system_has_linked_apo_or_pred\"]).system_id.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Selecting final dataset\n",
    "Let's select only the set that has linked structures for flexible docking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plindex_final_df = plindex_split_1P1L_links[\n",
    "    (plindex_split_1P1L_links.system_has_linked_apo_or_pred) & (plindex_split_1P1L_links.split != \"removed\")\n",
    "]\n",
    "plindex_final_df.groupby([\"split\", \"system_has_linked_apo_or_pred\"]).system_id.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plindex_final_df[[\"ligand_rdkit_canonical_smiles\", \"filename\"]].iloc[0].filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using _PLINDER_ API to load dataset by split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plinder.core import PlinderDataset\n",
    "from plinder.core.loader import get_model_input_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = PlinderDataset(\n",
    "    df=plindex_final_df,\n",
    "    split=\"train\",\n",
    "    num_alternative_structures=2,\n",
    "    file_paths_only=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: function `get_model_input_files` accepts `split =` \"train\", \"val\" or \"test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_dataset = get_model_input_files(\n",
    "    plindex_final_df,\n",
    "    split = \"val\",\n",
    "    max_num_sample = 10,\n",
    "    num_alternative_structures = 1,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "note, if files not already available this downloads them to `~/.local/share/plinder/{PLINDER_RELEASE}/{PLINDER_ITERATION}` directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using _PLINDER_ clusters in sampling\n",
    "- #TODO: Example how to create a simple diversity sampler based on cluster labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define diversity sampler function\n",
    "Here, we have provided an example of how one might use `torch.utils.data.WeightedRandomSampler`. However, users are free to sample diversity any how they see fit. For this example, we are going to use the sample dversity based on the `cluster` column in the splits dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
