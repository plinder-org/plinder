{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading data for training ML models\n",
    "This goal of this tutorial is to provide a simple hands-on example of how a user might load PLINDER dataset in prepparation for training machine learning models. Here, we are going to show an example of how to get the fasta sequence and smiles proteins and ligands. In the process, we will show:\n",
    "- How load use `PlinderDataset` to load data direclty from a given split dataframe\n",
    "- How one might use a diversity sampler along with the dataset loader\n",
    "- Extract specific data one might want to use for training specific ML model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load dataset\n",
    "We recommend users interact with the dataset using PLINDER Python API. class {class}`PlinderDataset` is the primary method of access data.\n",
    "\n",
    "- `df`: the split to use\n",
    "- `split`: the split to sample from\n",
    "- `file_with_system_ids`: path to a file containing a list of system ids (default: full index)\n",
    "- `store_file_path`: if True, include the file path of the source structures in the dataset\n",
    "- `load_alternative_structures`: if True, include alternative structures in the dataset\n",
    "- `num_alternative_structures`: number of alternative structures (apo and pred) to include\n",
    "\n",
    "Below, we are providing a function to access class {class}`PlinderDataset` and samples from protein-ligand similarity cluster based on sampling user-defined function `sampler_func` via a warpper function `load_dataset_path`. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from typing import Callable\n",
    "import pandas as pd\n",
    "from plinder.core import get_split, PlinderDataset\n",
    "from pathlib import Path\n",
    "from plinder.core.utils.log import setup_logger\n",
    "\n",
    "LOG = setup_logger(__name__)\n",
    "\n",
    "def load_dataset_path(\n",
    "        split_df: pd.DataFrame,\n",
    "        split: str = \"train\",\n",
    "        split_parquet_path: Path | None =None,\n",
    "        store_file_path: bool = True,\n",
    "        load_alternative_structures: bool = True,\n",
    "        num_alternative_structures: int = 1,\n",
    "        max_num_sample: int | None = None,\n",
    "        sampler_func: Callable | None =None):\n",
    "    \"\"\"\n",
    "    Load dataset from splits dataframe\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    split_df : pd.DataFrame | None\n",
    "        the split to use\n",
    "    split : str\n",
    "        the split to sample from\n",
    "    file_with_system_ids : str | Path\n",
    "        path to a file containing a list of system ids (default: full index)\n",
    "    store_file_path : bool, default=True\n",
    "        if True, include the file path of the source structures in the dataset\n",
    "    num_alternative_structures : int, default=1\n",
    "        number of alternative structures (apo and pred) to include\n",
    "    max_num_sample: int | None, default = None\n",
    "        maximum number of sample to return\n",
    "    sampler_func: Callable | None, default=None\n",
    "        user-defined diversity sampler\n",
    "    \"\"\"\n",
    "    dataset = PlinderDataset(\n",
    "        df=split_df,\n",
    "        split=split,\n",
    "        split_parquet_path=split_parquet_path,\n",
    "        store_file_path=store_file_path,\n",
    "        num_alternative_structures=num_alternative_structures)\n",
    "    if sampler_func is not None:\n",
    "        sampler = sampler_func(split_df, split)\n",
    "        for i in sampler:\n",
    "            if (max_num_sample is not None) & ((i+1)%max_num_sample == 0):\n",
    "                break\n",
    "            try:\n",
    "                yield dataset[i]\n",
    "            except Exception as e:\n",
    "                LOG.warn(e)\n",
    "    else:\n",
    "        for i in range(len(dataset)):\n",
    "            if (i+1)%max_num_sample == 0:\n",
    "                break\n",
    "            try:\n",
    "                yield dataset[i]\n",
    "            except Exception as e:\n",
    "                LOG.warn(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define diversity sampler function\n",
    "Here, we have provided an example of how one might use `torch.utils.data.WeightedRandomSampler`. However, users are free to sample diversity any how they see fit. For this example, we are going to use the sample dversity based on the `cluster` column in the splits dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_sampler(split_df, split=\"train\"):\n",
    "    from torch.utils.data import WeightedRandomSampler\n",
    "    split_df = split_df[split_df.split == split]\n",
    "    cluster_counts = (split_df[\"cluster\"].value_counts().rename(\"cluster_count\"))\n",
    "    split_df = split_df.merge(\n",
    "        cluster_counts,\n",
    "        left_on=\"cluster\",\n",
    "        right_index=True)\n",
    "    cluster_weights = 1.0 / split_df.cluster_count.values\n",
    "    return WeightedRandomSampler(\n",
    "        weights=cluster_weights,\n",
    "        num_samples=len(cluster_weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract specific molecular format needed for training\n",
    "The function `get_model_input` wraps it all together, allowing us to extract the sequence fasta and smiles needed for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_input(\n",
    "        split=\"train\",\n",
    "        sampler_func=None,\n",
    "        max_num_sample=10,\n",
    "        num_alternative_structures=1):\n",
    "    from rdkit import Chem\n",
    "    split_df = get_split()\n",
    "    training_set = load_dataset_path(\n",
    "            split_df,\n",
    "            split=split,\n",
    "            split_parquet_path=None,\n",
    "            store_file_path=True,\n",
    "            num_alternative_structures=num_alternative_structures,\n",
    "            max_num_sample=max_num_sample,\n",
    "            sampler_func=sampler_func)\n",
    "    protein_ligand_path = []\n",
    "    for data in training_set:\n",
    "        system_dir = Path(data[\"path\"]).parent\n",
    "        protein_fasta = system_dir / \"sequences.fasta\"\n",
    "        for ligand_sdf in (system_dir / \"ligand_files/\").glob(\"*sdf\"):\n",
    "            smiles = Chem.MolToSmiles(next(Chem.SDMolSupplier(ligand_sdf)))\n",
    "            protein_ligand_path.append((protein_fasta, smiles))\n",
    "    return protein_ligand_path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample diverse training set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get splits dataframe\n",
    "split_df = get_split()\n",
    "# Sample training set\n",
    "training_set = get_model_input(split=\"train\", sampler_func=make_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect result\n",
    "for i in training_set:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get validation set without cluster sampling\n",
    "Here, we will show how to get validation set without cluster sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample validation set without cluster sampling\n",
    "validation_set = get_model_input(split=\"val\", sampler_func=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect result\n",
    "for i in validation_set:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
