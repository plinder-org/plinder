{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Access _PLINDER_ data for training ML models\n",
    "\n",
    "The goal of this tutorial is to provide a simple and simple hands-on demo for a new user to access _PLINDER_ dataset in prepparation for training machine learning models.\n",
    "\n",
    "Here, we are going to demonstrate how to get the key input data:\n",
    "- protein receptor fasta sequence\n",
    "- small molecules ligand SMILES string\n",
    "- access to linked _apo_ and _pred_ structure\n",
    "\n",
    "\n",
    "In the process, we will show:\n",
    "- How to download the _PLINDER_ data\n",
    "- How to query _PLINDER_ index and splits to select relevant data using `plinder.core` API\n",
    "- Extract task-specific data one might want to use for training a task-specific ML model, eg. one protein, one ligand\n",
    "- How to use `plinder.core` API and `PlinderDataset` class to supply dataset inputs for `train` or `val` splits\n",
    "- Load linked `apo` and `pred` structures\n",
    "- Example how to create a simple diversity sampler based on cluster labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download _PLINDER_\n",
    "To download, run: `plinder_download --release 2024-06 --iteration v2 --yes` <br>\n",
    "This will download and unpack all neccesary files. For more information on download check out [Dataset Tutorial](https://plinder-org.github.io/plinder/tutorial/dataset.html#getting-the-data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Loading _PLINDER_\n",
    "\n",
    "We recommend users interact with the dataset using _PLINDER_ Python API.\n",
    "\n",
    "To install the API run: ``pip install plinder[loader]``. If you are using zsh terminal, you will have to quote the package like ``\"plinder[loader]\"``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from __future__ import annotations\n",
    "import os\n",
    "import pandas as pd\n",
    "from plinder.core.scores import query_index\n",
    "\n",
    "os.environ[\"PLINDER_OFFLINE\"] = \"true\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load _PLINDER_ index with selected columns from annotations table\n",
    "For a full list with descriptions, please refer to [docs](https://plinder-org.github.io/plinder/dataset.html#annotation-tables-index)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get plinder index with selected annotation columns specified\n",
    "plindex = query_index(\n",
    "    columns=[\"system_id\", \"ligand_id\",\n",
    "             \"ligand_rdkit_canonical_smiles\", \"ligand_is_ion\",\n",
    "             \"ligand_is_artifact\", \"system_num_ligand_chains\",\n",
    "             \"system_num_neighboring_protein_chains\"],\n",
    "    filters=[\n",
    "        (\"system_type\", \"==\", \"holo\"),\n",
    "        (\"system_num_neighboring_protein_chains\", \"<=\", 5)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plindex.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display number of system neighboring protein chains\n",
    "plindex.groupby(\"system_num_neighboring_protein_chains\").system_id.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting specific data using _PLINDER_ annotations\n",
    "As we can see just from the data tables above - a significant fraction of _PLINDER_ systems contain complex multi protein chain systems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task specific selection\n",
    "If we would like to focus on single protein and single ligand systems for training, we can use the annotated columns to filter out systems that:\n",
    "- contain only one protein chain\n",
    "- only one ligand\n",
    "\n",
    "Remember: In _PLINDER_ artifacts and (single atom) ions are also included in the index if they are part of the pocket.\n",
    "- We can use columns `ligand_is_ion` and `ligand_is_artifact` to only select \"proper\" ligands.\n",
    "\n",
    "Let's find out how many annotated ligands are \"proper\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define \"proper\" ligands that are not ions or artifacts\n",
    "plindex[\"ligand_is_proper\"] = (\n",
    "    ~plindex[\"ligand_is_ion\"] & ~plindex[\"ligand_is_artifact\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plindex.groupby(\"ligand_is_proper\").system_id.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### User choice\n",
    "\n",
    "The annotations table gives flexibility to choose the systems for training:\n",
    "- One could strictly choose to use only the data that contains single protein single ligand systems\n",
    "- Alternatively one could expand the number of systems to include systems containing single proper ligands, and optionally ignore the artifacts and ions in the pocket\n",
    "\n",
    "Let's compare the numbers of such systems!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create mask for single receptor single ligand systems\n",
    "systems_1p1l = (plindex[\"system_num_neighboring_protein_chains\"] == 1) & (plindex[\"system_num_ligand_chains\"] == 1)\n",
    "\n",
    "# make count of these \"proper\" ligands per system\n",
    "plindex[\"system_proper_num_ligand_chains\"] = plindex.groupby(\"system_id\")[\"ligand_is_proper\"].transform(\"sum\")\n",
    "\n",
    "# create mask only for single receptor single \"proper\" ligand systems\n",
    "systems_proper_1p1l = (plindex[\"system_num_neighboring_protein_chains\"] == 1) & (plindex[\"system_proper_num_ligand_chains\"] == 1) & plindex[\"ligand_is_proper\"]\n",
    "\n",
    "print(f\"Number of single receptor single ligand systems: {sum(systems_1p1l)}\")\n",
    "print(f\"Number of single receptor single \\\"proper\\\" ligand systems: {sum(systems_proper_1p1l)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see - the second choice can provide up to 20% more data for training, however, the caveat is that some of the interactions made by artifacts or ions may influence the binding pose of the \"proper\" ligand. The user could come up with further strategies to filtering using annotations table or external tools, but this is beyond the scope of this tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading splits\n",
    "\n",
    "Now, after curating the systems of interest, let's have a look at the splits using _PLINDER_ API.\n",
    "\n",
    "- How to use {mod}`plinder.core` API and class {class}`PlinderDataset` class to supply dataset inputs for `train` or `val` splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plinder.core import get_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting the splits\n",
    "\n",
    "The `get_split` function provided the current _PLINDER_ split, the detailed description of this DataFrame is provide in the [dataset documentation](https://plinder-org.github.io/plinder/dataset.html#splits-splits), but for our practical purposes we are mostly interested in `system_id` and `split` that assigns each of our systems to a specific split category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the current plinder split\n",
    "split_df = get_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some specific method developers working on _flexible_ docking may also find handy the annotation column `system_has_apo_or_pred` indicating if the system has available `apo` or `pred` linked structures (see later)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_df.groupby([\"split\", \"system_has_apo_or_pred\"]).system_id.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For simplicity let's merge plindex and split DataFrames into one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge to a single DataFrame\n",
    "plindex_split = plindex.merge(split_df, on=\"system_id\", how=\"left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting links to `apo` or `pred` structures\n",
    ":::{currentmodule} plinder.core:::\n",
    "For users interested in including `apo` and `pred` structures in their workflow, all the information needed can be obtained from the function {func}`query_links`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plinder.core.scores import query_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links_df = query_links(\n",
    "    columns=[\"reference_system_id\", \"id\", \"sort_score\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the table is sorted by `sort_score` that is resolution for `apo`s and `plddt` for `pred`s. The `apo` or `pred` is specified in the additionally added `kind` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If a user wants to consider only one linked structure per system - we can easily drop duplicates, first sorting by `sort_score`. Using this priority score, `pred` structures will not be used unless there is no `apo` available. Alternative can be achieved by sorting with `ascending=False`, or filtering by `kind==\"pred\"` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_links_df = links_df.sort_values(\"sort_score\", ascending=True).drop_duplicates(\"reference_system_id\")\n",
    "single_links_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have links to `apo` / `pred` structures, we can see how many of those are available for our single protein single ligand systems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plindex_split[systems_1p1l].groupby([\"split\", \"system_has_apo_or_pred\"]).system_id.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plindex_split_1p1l_links = plindex_split[systems_1p1l].merge(single_links_df, left_on=\"system_id\", right_on=\"reference_system_id\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's check how many systems have linked structures\n",
    "plindex_split_1p1l_links['system_has_linked_apo_or_pred'] = ~plindex_split_1p1l_links.filename.isna()\n",
    "plindex_split_1p1l_links.groupby([\"split\", \"system_has_linked_apo_or_pred\"]).system_id.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Selecting final dataset\n",
    "Let's select only the set that has linked structures for flexible docking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plindex_final_df = plindex_split_1p1l_links[\n",
    "    (plindex_split_1p1l_links.system_has_linked_apo_or_pred) & (plindex_split_1p1l_links.split != \"removed\")\n",
    "]\n",
    "plindex_final_df.groupby([\"split\", \"system_has_linked_apo_or_pred\"]).system_id.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plindex_final_df[[\"ligand_rdkit_canonical_smiles\", \"filename\"]].iloc[0].filename"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using _PLINDER_ API to load dataset by split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plinder.core import PlinderDataset\n",
    "from plinder.core.loader import get_model_input_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = PlinderDataset(\n",
    "    df=plindex_final_df,\n",
    "    split=\"train\",\n",
    "    num_alternative_structures=2,\n",
    "    file_paths_only=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{currentmodule} plinder.core:::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{note} function {func}`get_model_input_files` accepts `split =` \"train\", \"val\" or \"test\":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_dataset = get_model_input_files(\n",
    "    plindex_final_df,\n",
    "    split = \"val\",\n",
    "    max_num_sample = 10,\n",
    "    num_alternative_structures = 1,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "note, if files not already available this downloads them to `~/.local/share/plinder/{PLINDER_RELEASE}/{PLINDER_ITERATION}` directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect data\n",
    "sample_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using _PLINDER_ clusters in sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define diversity sampler function\n",
    ":::{currentmodule} plinder.core:::\n",
    "\n",
    "In general, diversity can be sampled using cluster information described [here](https://plinder-org.github.io/plinder/dataset.html#clusters-clusters). All cluster information can easily be added to `plindex` by calling the following function {func}`get_extended_plindex`. <br>\n",
    "Here, we have provided an example of how one might use the function `get_diversity_samples` which is based on  `torch.utils.data.WeightedRandomSampler`. This example is meant for demonstration purposes and users are encouraged to come up with sampling strategy that suits their need. <br>\n",
    "For this example, we are going to use the sample dversity based on the following parameters:\n",
    "`metric=\"pli_qcov\"`,  `threshold=70` and `cluster=communities`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plinder.core.loader import  get_diversity_samples\n",
    "from plinder.core import get_extended_plindex\n",
    "from typing import Literal\n",
    "\n",
    "# Example sampler function, this is for demonstration purposes,\n",
    "# users are advised to write a sampler that suit their need\n",
    "def get_diversity_samples(\n",
    "    split_df: pd.DataFrame,\n",
    "    split: Literal[\"train\", \"val\", \"test\"] = \"train\",\n",
    "    metric: str = \"pli_qcov\",\n",
    "    threshold: int = 70,\n",
    "    cluster_type: str = \"communities\"):\n",
    "\n",
    "    from torch.utils.data import WeightedRandomSampler\n",
    "\n",
    "    split_df = split_df[split_df.split == split]\n",
    "    cluster_counts = (split_df[f\"{metric}__{threshold}__{cluster_type}\"]\\\n",
    "                      .value_counts().rename(\"cluster_count\"))\n",
    "    split_df = split_df.merge(\n",
    "        cluster_counts,\n",
    "        left_on=f\"{metric}__{threshold}__{cluster_type}\",\n",
    "        right_index=True)\n",
    "    split_df.reset_index(inplace=True)\n",
    "    cluster_weights = 1.0 / split_df.cluster_count.values\n",
    "    sampler = WeightedRandomSampler(\n",
    "        weights=cluster_weights,\n",
    "        num_samples=len(cluster_weights))\n",
    "    sampler_index = [i for i in sampler]\n",
    "    return split_df.loc[\n",
    "        tuple(sampler_index),\n",
    "        (\"system_id\", \"split\")].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note, this takes a couple of minutes to run\n",
    "plindex_extended = get_extended_plindex()\n",
    "cluster_columns = [\"system_id\"] + [col for col in plindex_extended.columns \\\n",
    "                                   if (\"communities\" in col) or (\"component\" in col)]\n",
    "plindex_with_clusters = plindex_final_df.merge(plindex_extended[cluster_columns], on=\"system_id\", how=\"left\")\n",
    "sampled_df =  get_diversity_samples(split_df=plindex_with_clusters,\n",
    "                                    metric=\"pli_qcov\",\n",
    "                                    threshold=70,\n",
    "                                    cluster_type=\"communities\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The returned dataframe could then be passed to {func}`get_model_input_files` the same way `plindex_final_df` was used above."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
